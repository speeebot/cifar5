# -*- coding: utf-8 -*-
"""cifar5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_tWll4-lCrmAbIRmtXUTcquLoPsISMvR

Convolutional Neural Network Assignment
 
You will use this jupyter notebook called cifar5ML23.ipynb from files.  The dataset consists of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.
This code pulls 5 animal classes from the cifar10 data set.  All but frog. Classes given below. You have 25,000 train examples and 5,000 test examples. From the 25,000 training examples, we randomly extracted 10% (2,500) as a validation set.  Weights are saved when the performance on the validation set improves.  The set-up is to use the best weight set.  Ideally, you do not test on the test data until you are done improving validation accuracy.  You must tell us exactly what you have done in regards to testing and provide a link to the best validation weight set.

 Also below is how to put the needed data in the right directory.   The code you are given provides an accuracy of a bit less than 70% as is.
Note that on GPU’s with tensorflow and the same initialization you get different results (usually) for 2 runs.  Your project has 2 parts.  The first is make some change(s) to the code to improve performance and show over at least two runs averaged.  Nothing in the code is optimal so this should be easy enough to do.  You must explain what you changed, provide the results and explain why you believe it improved performance.  A very simple change to improve will get provide average credit.

This dataset has been used a lot and there are other implementations available on the internet.  It will be most beneficial to your learning to make your own changes and evaluate.  IF you simply must use other code you MUST carefully cite it and provide a working URL.  

Your second task is to build the most accurate convolution only model you can.  This means remove the fully connected hidden layer.  That is the code that says flatten followed by a dense layer.  You will want to look at keras.io documentation to follow commands in the code provided that are relevant to performance.  Again, you must explain what you have done, why and why you think it helped.    

The classes are as follows with * meaning used here:
 0 : airplane
1 : automobile
2 : bird *
3 : cat  *
4 : deer *
5 : dog  *
6 : frog
7 : horse *
8 : ship
9 : truck

Some possible questions to answer:
    1. Do more epochs help?  Why or why not?
    2. Does regularization help? 
    3. Does a different network configuration help?
    4. Does learning rate help?
    5. Does momentum help?
    6. Does batch size help?
    7. Do any of the things you did slow down learning?
    8. Is the fully convolutional network (no fully connected layer before the output layer) performing, better, worse or the same?
    9. Does having less/more parameters help accuracy?
   10. Does loss function, activation function, optimizer, validation set use, etc. help?
    
   Always add your best explanation to the above.  Be sure to report accuracy.
"""

'''
-------------------------------------------------------------------------CHANGE HISTORY-----------------------------------------------------------------
MODEL#	EPOCHS	LEARNING RATE	BATCH SIZE	OPTIMIZER	DATA AUGMENTATION	FULLY CONNECTED LAYER	VALIDATION ACCURACY	TEST ACCURACY	TEST LOSS	NOTES
D	15	0.0001	16	RMSprop	N	N	0.6968	0.6877999902	0.8297593594	baseline design, 68.7% accuracy
1	30	0.0001	16	RMSprop	N	N	0.7552	0.7447999716	0.7002073526	increase epochs from 15->30
2	50	0.0001	16	RMSprop	N	N	0.7772	0.7630000114	0.6508851051	increase epochs from 30->50
3	50	0.001	16	RMSprop	N	N	0.7384	0.7238000035	0.7996025681	lower learning rate from 0.0001->0.001
4	50	0.001	16	RMSprop	Y	N	0.7172	0.7099999785	0.7890551686	using data augmentation
5	50	0.0001	16	RMSprop	Y	N	0.7904	0.7857999802	0.612046361	RMSprop with learning rate back to 0.0001, using data augmentation
6	50	0.0001	16	RMSprop	Y	Y	0.802	0.7960000038	0.5818503499	added fully connected layer
7	50	0.0001	16	Adam	Y	Y	0.8196	0.818599999	0.5012459755	RMSprop->Adam, Adjusting learning rate might help. Also could try increasing batch size.
8	50	0.0001	16	Adam	Y	N	0.7892	0.7789999843	0.6055569053	removed fully connected layer
9	50	0.001	64	Adam	Y	Y	0.8628	0.8593999743	0.4008983374	64 batch size, 0.001 learning rate with Adam optimizer
10	50	0.001	64	Adam	Y	N	0.858	0.8461999893	0.4134301841	removed fully connected layer
11	50	0.001	64	SGD	Y	Y	0.846	0.8361999989	0.4581479132	SGD with 0.9 momentum,
12	50	0.001	64	SGD	Y	N	0.8224	0.8216000199	0.5083597302	removed fully connected layer
13	100	0.001	64	SGD	Y	Y	0.8772	0.8704000115	0.3799341917	100 epochs, using VGG blocks, he_uniform weight initialization, batch normalization
14	100	0.001	64	SGD	Y	N	0.8584	0.8555999994	0.4097082913	removed fully connected layer
15	100	0.001	64	Adam	Y	Y	0.89	0.8731999993	0.368781805	Adam with 100 epochs
16	100	0.001	64	Adam	Y	N	0.8792	0.8690000176	0.3891464174	removed fully connected layer
17	100	0.001	32	Adam	Y	Y	0.8868	0.8873999715	0.3196287751	batch size 32
18	100	0.001	32	Adam	Y	N	0.8864	0.877399981	0.3721897304	removed fully connected layer

17 	100	0.001	32	Adam	Y	Y	0.8868	0.8758000135	0.3523483872	second run for averaging
18 	100	0.001	32	Adam	Y	N	0.886	0.8755999804	0.3672086596	second run for averaging

The best performing fully connected and convolution only models were models 17 and 18, respectively. 
The first change was to increase the epochs from the baseline 15 to 30. This showed a notable increase
in accuracy as the model had not converged with only 15 epochs. Then, the epochs were increased to 50; 
There was still a gain in accuracy, but smaller. Next, the learning rate was increased to 0.001. 
This worsened the accuracy of the model. Using data augmentation with the original 0.0001 learning rate 
gave the best accuracy so far of 78.5%. Data augmentation creates copies of slightly altered versions of 
the images in the dataset, in an attempt to avoid overfitting and promote learning. For all of the tests 
moving forward, data augmentation was used as it seemed to only help. Up until this point, the model was 
convolutions only. Then the fully connected hidden layer was uncommented, and an increase in accuracy 
was observed from the previous 78.5% to the new best of 79.6% in model 6.

For the fully connected hidden layer model (7), Adam optimizer was used with a learning rate of 0.0001 at first.
This resulted in a further increase in accuracy to 81.8%. In the convolution only model (8), there was no improvement
over model (6), which was still the best convolution only model so far. In the Keras documentation for Adam, 
the default learning rate is 0.001. Changed the learning rate to 0.001 alongside an increase in batch size from
16 to 64. Increasing batch size seemed to speed up learning a little bit. This resulted in a new best accuracy
of 85.9% for the fully connected model (9) and 84.6 for the convolution only model (10). Next, SGD with 
momentum of 0.9 was tested. This gave worse accuracy, but not by much (model 11 and 12). This is where
it was decided to change the model architecture to VGG blocks (https://arxiv.org/abs/1409.1556v6). Using 3 VGG 
blocks, with he_uniform weight initialization and batch normalization to help with convergence and overfitting issues, 
the model was trained over 100 epochs. This gave pretty good results with accuracy of 87% for the fully connected model (13) 
and 85.8% for the convolution only model (14). Lastly, it was decided to combine these results and use Adam optimizer at
0.001 learning rate, with the VGG architecture. After lowering the batch size from 64 to 32, the final test accuracy achieved
with all changes and improvements was 88.1% for the fully connected model (17) and 87.6% for the convolution only model (18), 
averaged over 2 runs.


1. Do more epochs help? Why or why not? 
    Increasing the epochs helped the network learn better. With the original 15 epochs, the model did not seem to converge. 
    And after changing the model design, more epochs seemed necessary.
2. Does regularization help? 
    Using dropout definitely helps the model avoid overfitting by "dropping out" a fraction of neurons during training; 
    This allows the model to better generalize.
3. Does a different network configuration help? 
    Yes, using a network with VGG style blocks (https://arxiv.org/abs/1409.1556v6) can improve performance over the original network design.
    VGG blocks use 3x3 filter, where the number of filters increases with each block. After 2 or 3 convolutions, a downsampling layer 
    is added via max pooling.
4. Does learning rate help? 
    Fine tuning the learning rate helped, but is dependent on the optimizer that is used. Adam seemed to performan best with 0.001, whereas 
    RMSprop had best the performance at 0.0001.
5. Does momentum help? 
    Yes. Momentum can help optimization algorithms like stochastic gradient descent (SGD) converge toward the optimal solution by
    dampening oscillations.
6. Does batch size help? 
    Increasing batch size can help, but not always. Larger batch size can lead to a better gradient estimate in some instances (less noise), 
    but also consumes more memory. Having a smaller batch size adds noise to the estimate, which can help avoid convergence at non-optimal 
    local minima. However, too low of batch size can introduce too much noise to be useful.
7. Do any of the things you did slow down learning? 
    Increasing the size of the network slowed down learning. Also, not using batch normalization decreased the learning speed.
    Finally, using a smaller batch size was slower.
8. Is the fully convolutional network (no fully connected layer before the output layer) performing, better, worse or the same? 
    Performance after removing the fully connected layer is either similar, or in most tested designs, notably worse.
9. Does having less/more parameters help accuracy? 
    Having more parameters helps learning to a degree. It is possible to have too many parameters, which can lead to overfitting. 
    A larger model with regularization techniques can perform noticably better than a smaller model.
10. Does loss function, activation function, optimizer, validation set use, etc. help?
     Using the correct loss function can improve performance, in this case categorical cross entropy is used because it is a multiclass 
     classification problem (one hot encoded). ReLU is used for activation function because it generally performs as good, or better
     than other activations. For optimizer, RMSprop was the baseline and seemed to perform okay. However, SGD with 0.9 momentum performed
     slightly better. Adam performed the best overall, likely because it combines the best of RMSprop with momentum.
'''

from __future__ import print_function
from __future__ import absolute_import
from __future__ import division

# Tensorflow/keras code modified by L.O. Hall 8/7/20 to load 5 of 6 animal
# classes for training (no frog) and put in a notebook. 
# Updated to have validation set by Kin Ng 4/4/23

import tensorflow as tf
import tensorflow.keras as keras
from keras.utils import get_file 

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.utils import to_categorical

import os
import math
import numpy as np

from tensorflow.keras import backend as K
from tensorflow.keras.datasets import cifar10
from tensorflow.python.util.tf_export import tf_export

from sklearn.model_selection import train_test_split # for generating validation data

'''try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Device:', tpu.master())
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.TPUStrategy(tpu)
except:
    strategy = tf.distribute.get_strategy()
print('Number of replicas:', strategy.num_replicas_in_sync)'''

""" Define hyperparameter values and other relevant options """
batch_size = 32
learning_rate = 0.001
epochs=100
data_augmentation = True
#opt = tf.keras.optimizers.SGD(lr=learning_rate, momentum=0.9)
#opt = tf.keras.optimizers.RMSprop(lr=learning_rate)
opt = tf.keras.optimizers.Adam(lr=learning_rate)

num_classes = 10 # Do not change

save_dir = os.path.join(os.getcwd(), 'saved_models') # Directory to save trained model
model_name = 'keras_cifar10_trained_model.h5' # Name of trained model. Change accordingly if running multiple experiments
model_path = os.path.join(save_dir, model_name) 

# Create directory to save models, if it doesn't exist
if not os.path.isdir(save_dir):
    os.makedirs(save_dir)

# Callback for saving best epoch checkpoint weights
checkpoint = keras.callbacks.ModelCheckpoint(
  filepath=model_path, 
  monitor='val_accuracy', 
  verbose=1, 
  save_best_only=True
)
callbacks = [checkpoint]

def load_data5():
  """Loads CIFAR10 dataset. However, just 5 classes, all animals except frog
  Returns:
      Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.
  """
  dirname = 'cifar-10-batches-py'
  origin = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'
  path = get_file(dirname, origin=origin,  untar=True)
  path= './cifar-10-batches-py'
  (x_train, y_train), (x_test, y_test) = cifar10.load_data()
  tclasscount=np.zeros((10,), dtype=int)
  
  for i in range(0, len(y_test)):
    tclasscount[y_test[i][0]]= tclasscount[y_test[i][0]] + 1
  num_train_samples = 50000
  num_5_class = 25000
  num_5_test = 5000 
  print('x_train shape orig:', x_train.shape)
  print('More:', x_train.shape[1:])
  print('y_test shape',y_test.shape)

  x5_train = np.empty((num_5_class, 32, 32, 3), dtype='uint8')
  y5_train = np.empty((num_5_class,), dtype='uint8')

  count=0
  for i in range(0, len(y_train)):
   if (y_train[i][0] == 2) or (y_train[i][0] == 3) or (y_train[i][0] == 4) or (y_train[i][0] == 5) or (y_train[i][0] == 7):
    x5_train[count]=x_train[i]
    y5_train[count]=y_train[i]
    count=count+1
   
  # find test data of interest
  count=0
  x5_test=np.empty((num_5_test, 32, 32, 3), dtype='uint8')
  y5_test= np.empty((num_5_test,), dtype='uint8')

  for i in range(0, len(y_test)):
   if (y_test[i][0] == 2) or (y_test[i][0] == 3) or (y_test[i][0] == 4) or (y_test[i][0] == 5) or (y_test[i][0] == 7):
    x5_test[count]=x_test[i]
    y5_test[count]=y_test[i]
    count=count+1

  return (x5_train, y5_train), (x5_test, y5_test)

""" Load train and test Data"""
(x_train, y_train), (x_test, y_test) = load_data5()

""" Generate validation data """
 # Use random seed for reproducibility and stratified sampling. Validation is 10% out of train data
x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=0, stratify=y_train)

print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')
print(x_valid.shape[0], 'validation samples')
print(y_valid.shape[0], 'Validation y')
print(y_train.shape[0], 'Train y')
print(y_test.shape[0], ' Test y')

steps_for_epoch = math.ceil(x_train.shape[0] / batch_size)
print('num classes',num_classes)
print('y_train',y_train)
print('y_valid',y_valid)
print('y_test',y_test)

# Convert class vectors to binary class matrices.
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)
y_valid = to_categorical(y_valid, num_classes)

""" Model Design - You must make some change(s) to improve performance """
model = Sequential()

model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
model.add(BatchNormalization())
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))

model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.3))

model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.4))

model.add(Flatten())

model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Dropout(0.5))

model.add(Dense(10, activation='softmax'))

print(model.summary())

# compile model
model.compile(optimizer=opt, 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

# Normalize data
x_train = x_train.astype('float32')
x_valid = x_valid.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_valid /= 255
x_test /= 255

""" Model Training """
if not data_augmentation:
    print('Not using data augmentation.')
    model.fit(x_train, y_train,
              batch_size=batch_size,
              epochs=epochs,
              validation_data=(x_valid, y_valid),
              shuffle=True,
              callbacks=callbacks)
else:   # THIS CODE NOT TESTED IN TENSORFLOW 2.0.  IT IS AS IS!!!
    print('Using real-time data augmentation.')
    # This will do preprocessing and realtime data augmentation:
    datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        zca_epsilon=1e-06,  # epsilon for ZCA whitening
        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
        # randomly shift images horizontally (fraction of total width)
        width_shift_range=0.1,
        # randomly shift images vertically (fraction of total height)
        height_shift_range=0.1,
        shear_range=0.,  # set range for random shear
        zoom_range=0.,  # set range for random zoom
        channel_shift_range=0.,  # set range for random channel shifts
        # set mode for filling points outside the input boundaries
        fill_mode='nearest',
        cval=0.,  # value used for fill_mode = "constant"
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False,  # randomly flip images
        # set rescaling factor (applied before any other transformation)
        rescale=None,
        # set function that will be applied on each input
        preprocessing_function=None,
        # image data format, either "channels_first" or "channels_last"
        data_format=None,
        # fraction of images reserved for validation (strictly between 0 and 1)
        validation_split=0.0)

    # Compute quantities required for feature-wise normalization
    # (std, mean, and principal components if ZCA whitening is applied).
    datagen.fit(x_train)

    # Fit the model on the batches generated by datagen.flow().
    model.fit(datagen.flow(x_train, y_train,
                                     batch_size=batch_size),
                        steps_per_epoch = steps_for_epoch,
                        epochs=epochs,
                        workers=4,
                        validation_data=(x_valid, y_valid),
                        callbacks=callbacks)

# Save trained model and weights - if using callbacks, then this portion of code is not needed
# if not os.path.isdir(save_dir):
#     os.makedirs(save_dir)
# model_path = os.path.join(save_dir, model_name)
# model.save(model_path)
# print('Saved trained model at %s ' % model_path)

# Load saved model
model.load_weights(model_path)

# Score trained model.
# DO NOT DO THIS UNTIL DONE GETTING BEST VALIDATION ACCURACY.
scores = model.evaluate(x_test, y_test, verbose=1)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])